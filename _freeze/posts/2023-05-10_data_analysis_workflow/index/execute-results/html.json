{
  "hash": "2fa7dbc9a65d3ce85478372b260104d2",
  "result": {
    "markdown": "---\ntitle: \"General data analysis workflow\"\nsubtitle: | \n  A brief outline of our approach to data analysis in recent years \ndate: \"2023-05-10\"\ncategories: [Data analysis, Statistics]\nimage: \"img/statistical-rethinking.jpeg\"\ndraft: true\nauthor:\n  - name: Richard Ramsey\n    url: https://rich-ramsey.com/\n    affiliation: ETH Zurich\n    affiliation-url: https://ethz.ch/en.html\n    orcid: 0000-0002-0329-2112\n---\n\n\n### tl:dr\n\n-   Pre-register predictions and specific analyses in advance\n\n-   Put p values and Bayes factors in the bin\n\n-   Estimate parameters within (multi-level) regression models\n\n-   Describe the posterior distribution of these parameters and evaluate them in relation to your hypotheses\n\n### Disclaimer\n\nI am not a statistician and I am not saying anything new here. I'm just summarising a bunch of other people's suggestions.\n\nIf I have confused things, then that's a shame. Such is life. Please send me a note with constructive feedback.\n\nFinally, I outline one general approach to data analysis here. I have found it really refreshing compared to my past experiences and therefore I want to blog about it. But I am aware that there are many valid ways to analyse the same data, rather than there being one *correct* way. So take any suggestions below in that spirit.\n\n### Objective\n\nI want to outline my lab's recent approach to data analysis because others may find it helpful. It is a relatively high-level overview, rather than a step-by-step tutorial.\n\n### Intended audience\n\nNewbies who are looking to move towards using multi-level regression models in a Bayesian workflow. It is not going to be newsworthy for those who already have experience with estimation approaches in multi-level regression.\n\n### My background\n\nMy background and training is in experimental psychology and cognitive neuroscience and my main research methods and statistics training will be very familiar to those from a psychology background. Each week we learned about a different \"test\". And each week involved some data, which was suitable to a different test. Week 1 starts with a t-test and you head towards more complex ANOVAs as the weeks move on. A version of this seems to be the cornerstone of most undergraduate and graduate statistics training worldover.\n\nAll you needed to do was pick the right test for the particular type of data and pray to the datagods that the p value would be \\< 0.05. If p was \\> 0.05, but still close (e.g., \\< 0.1), then ... say something vague like \"trending towards significance\". If p value was \\> 0.1 then just pretend that that means there is no real effect.\n\nI became tired of this and I was looking for something different.\n\nEnter Richard McElreath's book...\n\n### Main inspiration\n\nRichard McElreath's [textbook](https://xcelab.net/rm/statistical-rethinking/) and Solomon Kurz's [ebook](https://bookdown.org/content/4857/).\n\nWhat comes below takes its primary inspiration from these resources. I should say here that I am only presenting a small slice of what these books have to offer. And it may not be the most clear or coherent slice. So please read these books yourself for maximal benefit.\n\nTwo other noteworthy texts are as follows:\n\nJohn Krushke's Doing Bayesian Data Analysis [textbook](https://sites.google.com/site/doingbayesiandataanalysis/)\n\nBodo Winter's Statistics for Linguists [textbook](https://bodowinter.com/books.html)\n\n### Taking an estimation approach\n\nKrushke and Liddel [(2018)](https://link.springer.com/article/10.3758/s13423-016-1221-4) draw two distinctions with the following figure:\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](img/fig1.png){fig-align='left' width=50% height=50%}\n:::\n:::\n\n\nThey distinguish between hypothesis tests (top row) and Estimation with uncertainty (bottom row), both from a frequentist (left column) and Bayesian perspective (right column).\n\nTwo things are immediately striking about this. First, McElreath's entire textbook on Bayesian statistics does not involve Bayes factors (or p values, but that might have been expected). Second, taking an estimation approach is not tied to being a Bayesian. It can also be done in a frequentist framework[^1].\n\n[^1]: I think there are theoretical and practical reasons why a Bayesian approach has advantages, but that's not important here.\n\n#### What does this mean in practice and why do I find it appealing?\n\nWell, first of all, you can forget about the difficulties of interpreting the pesky p value or trying to benchmark Bayes factors. Both approaches may be perfectly fine to use (if used correctly), but you just don't need them under the estimation approach. Based on my stats background, that was a huge benefit because there was always something deeply unsatisfying about interpreting artificial bright marks (e.g., p \\< 0.05) or using pre-set benchmarks for how to interpret Bayes factors (e.g., [these scales](https://www.statlect.com/fundamentals-of-statistics/Jeffreys-scale)). Both of these approaches seemed to rely too heavily on categorical thinking (significant or not) or some framework for interpreting Bayes factors. All of which felt too certain and too artifical for my liking.\n\nInstead, the estimation approach is built around estimating the magnitude and uncertainty of parameters. I instinctively found this more satisfying. It seemed a simpler starting point to summarise the distribution of a parameter (e.g., give lower and upper bounds, as well as the peak density or some other summary value) without inherently giving special status or bright lines to certain values.\n\n#### What does estimation with uncertanty look like?\n\nSome examples may help here. Below you can see what estimation with uncertainty might look like in a frequentist and Bayesian workflows. \n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nFor illustration purposes, this is just simulated normal distribution data. And we pretend that there is just one parameter in the model (parameter1). Let's say paramter1 reflects the difference in points on a scale between two conditions (A and B). \n\nAs you can see, the point estimate is very close to zero and the interval estimates (which are 95% confidence / quantile intervals) extend from -2 to 2. As such, the best estimate for the difference between A and B is zero, with reasonable values ranging from -2 to 2.\n\n#### The benefit of Bayesian workflows - working in distributions\n\nAlthough estimation with uncertainty can be done in frequentist or Bayesian workflows, there is an important difference between the two parameter estimates above. One has a density plot and one does not. This is important because it is crucial for thinking about the inferences that you can draw. \n\nEven though most researchers (especially in psychology and human neuroscience) will be more familiar with frequentist thinking than Bayesian thinking, it turns out that understanding p values is not that straightforward (ref). And counterintuitively, it is more starightforward to understand the Bayesian estimation parameter above than the frequenist version (Krushke & Liddel, [2018](https://link.springer.com/article/10.3758/s13423-016-1221-4)). Try this on for size:\n\nThe frequentist interpretation:\n\n> If I repeat this study precisely an infinite number of times, and I calculate a 95% interval each time, then 95% of those intervals will contain the true parameter.\n\nThe Bayesian interpretation:\n\n> There is a 95% probability the parameter falls in this interval. \n\nIt seems the latter interpetation is the one that folks tend to think they are making and definitely want to be making. And that all stems from the fact that the Bayesian workflow is building a density distribution of plausible parameter values. After you have built the distribution, you can just summarise it in a straightforward manner using the tools of descriptive statistics.\n\n#### It is the structure of the model that matters, not the p value or Bayes factor\n\nIt is the integrity and structure of the model and the assumptions that the model embodies that take centre stage, rather than the bright lights of a p-value or Bayes factor value.\n\n### Build models incrementally towards the full model\n\nMy lab takes the \"keep it maximal\" approach to model building (Barr et al., [2013](https://www.sciencedirect.com/science/article/pii/S0749596X12001180)). The logic here is to include all the fixed and varying (random) effects that the design permits because it tends to be a better reflection of the reality we are trying to model. Read Dale Barr's paper for more details.\n\nTherefore, we start with an intercepts only model (i.e., one without any predictors) and build incrementally towards the full model. The full model being the one with the maximum number of fixed and varying effects that can be specified by the design.\n\n\n### Implementation: lme4 vs brms (and stan)\n\n[Brms](https://paul-buerkner.github.io/brms/) is a high-level front end package for [stan](https://mc-stan.org/). Brms uses the same syntax as [lme4](https://github.com/lme4/lme4) (which is very popular in psychology). And stan \n\n* Advantages: \n  + Stan has far fewer failures to converge, even with complex models.\n  + Much greater flexibility in model specification.\n  \n* Disadvantages: \n  + Computational power and time. Some Bayesian models can take a long time to run (hours, days, weeks). Only the advent of powerful desktop machines in the last 25 years or so make it even possible to run such models in a widespread manner. \n\n### Inference\n\nWe use two ways to make inferences.\n\n1.  Evaluate parameters of interest in the full model.\n\ngive an example of different posterior distributions and the extent to which they may or may not support your pre-registered analyses. e.g., two or three plots 1) overlapping zero, tightly and not so tightly. 2) Positive and tight and not so tight.\n\nThen describe the proportion of values that span a particular range and what that would mean for your hypothesis. e.g., values as low as blah and as high as blah are compatible with your model estimates.\n\n2.  Model comparison.\n\ngive an example. one way is to predict out-of-sample accuracy. Then compare between models. Penalty for added complexity.\n\nIn our experience in exp psych / cog neuro, model comparison has not been as helpful, but that's just our experience. They are both valuable and serve different purposes, so they may work for other labs more than others.\n\n**Then replicate:** At this point, I would then run a replication (or a replication and extension). By doing so, you avoid joining the \"Cult of the Isolated Study\", which is the trap of drawing conclusions from single experiments - see [here](https://doi.org/10.2307/2981525) and [here](https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1518264).\n\n### Pros and cons\n\nPros\n\n1.  One general and yet flexible approach to data analysis\n2.  Sophisticated and complex models via STAN\n\nCons\n\n1.  Some models can take quite some time to build (days, weeks, months?) and need greater computer power.\n\n### Implications\n\n* Compared to more conventional stats training \n  + Much more information is used and presented\n  + Assumptions reflect what we know about human cognition and behaviour i.e., complex, variable (across people and situations) and multi-levelled.\n  + Bespoke models need building and choices need justifying – so you gotta think a bit…\n\n* Implications for the replication crisis?\n  + There are fewer places to hide, due to a more complete presentation of information\n  + It changes what a replication attempt would even look like e.g., p < 0.05 is not enough\n  + More nuanced interpretations are required.\n  + It naturally encourages considerations regarding generalisability (across people, stimuli, places etc.) by virtue of taking a maximal varying structure approach.\n\n\n### And finally (but most importantly) back to McElreath...\n\nOne of the best things about McElreath's book is that you get to the end of all this complex code and stats stuff, which made my brain hurt at times, and then he says statistical models, however advanced and complex, are fundamentally limited and need to always be framed within the wider scientific context, such as ...\n\n-   The importance of theory\n-   Open data and materials\n-   Pre-registration\n-   Meta-analyses\n-   Computational modelling\n-   Data science and visualisation\n-   Experimental design\n-   And many, many more besides\n\nIt is a very refreshing and humbling perspective to remember how broad, deep and diverse scientific workflows should be.\n\nAnd it made me realise how traditional stats training had inadvertently led us to expect far too much certainty from statistical models. We were asking statistical models to provide a level of certainty that was just not possible. And so it was time for a re-set.\n\n### Where should I start?\n\nIf you're interested in jumping on the Bayesian estimation bandwagon, then I would do the following.\n\n1.  Read this book first by Bodo Winter. It provides the perfect intro to estimation approaches using R. It also uses a frequentist approach, which is valuable because folks are typically more familiar with frequentist approaches than Bayesian ones and it reinforces why there is nothing Bayesian per se about estimation approaches.\n\n2.  Read McElreath's book and watch his lectures. Take one chapter at a time and complete the code exercises. If you are more of a tidyverse person (like me) and want to use the lme4 syntax that brms provides, then follow alongside McElreath's textbook with Solomon Kurz's ebook and associated code. That's what I did.\n\n3.  Take a look at code examples online from other people's work. There are a bunch. You can find some on my [publications page](https://rich-ramsey.com/publications/publications.html) and [OSF page](https://osf.io/s67e2/). But better yet, find someone else who has more experience like Solomon Kurz and many, many more people out there.\n\n4.  If you get stuck, write a reproducible example on the brms forum. Folks are really helpful on this forum and the forum seems to be fairly active.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}