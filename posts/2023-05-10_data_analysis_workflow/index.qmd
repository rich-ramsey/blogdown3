---
title: "General data analysis workflow"
subtitle: | 
  A brief outline of our approach to data analysis in recent years 
date: "2023-05-10"
categories: [Data analysis, Statistics]
image: "img/statistical-rethinking.jpeg"
draft: true
author:
  - name: Richard Ramsey
    url: https://rich-ramsey.com/
    affiliation: ETH Zurich
    affiliation-url: https://ethz.ch/en.html
    orcid: 0000-0002-0329-2112
---

### tl:dr

-   Pre-register predictions and specific analyses in advance

-   Put p values and Bayes factors in the bin

-   Estimate parameters within (multi-level) regression models

-   Describe the posterior distribution of these parameters and evaluate them in relation to your hypotheses

### Disclaimer

I am not a statistician and I am not saying anything new here. I'm just summarising a bunch of other people's suggestions.

If I have confused things, then that's a shame. Such is life. Please send me a note with constructive feedback.

Finally, I outline one general approach to analyses data. I have found it really refreshing compared to my past experiences and therefore I want to encourage it. But I am very aware that there are many valid ways to analyse data, rather than one *correct* way. So take it in that spirit.

### Objective

I want to outline my lab's recent approach to data analysis because others may find it helpful. It is a relatively high-level overview, rather than a step-by-step tutorial.

### Intended audience

Newbies who are looking to move towards using multi-level regression models in a Bayesian workflow. It is not going to be newsworthy for those who already have experience with estimation approaches in multi-level regression.

### My background

My background and training is in experimental psychology and cognitive neuroscience and my main research methods and statistics training will be very familiar to those from a psychology background. Each week we learned about a different "test". And each week involved some data, which was suitable to a different test. Week 1 starts with a t-test and you head towards more complex ANOVAs. 

All you needed to do was pick the right test for the particular type of data and pray to the datagods that the p value would be \< 0.05. If p was \> 0.05, but still close (e.g., \< 0.1), then ... say something vague like "trending towards significance". If p was \> 0.1 then just pretend that that means there is no real effect.

I became tired of this and I was looking for something different.

Enter Richard McElreath's book...

### Main inspiration

Richard McElreath's [textbook](https://xcelab.net/rm/statistical-rethinking/) and Solomon Kurz's [ebook](https://bookdown.org/content/4857/).

What comes below takes its primary inspiration from these resources. I should say here that I am only presenting a small slice of what these books have to offer. And it may not be the most clear or coherent slice. So please read these books yourself for maximal benefit.

Two other noteworthy texts are as follows:

John Krushke's Doing Bayesian Data Analysis [textbook](https://sites.google.com/site/doingbayesiandataanalysis/)

Bodo Winter's Statistics for Linguists [textbook](https://bodowinter.com/books.html)

### Taking an estimation approach

Krushke and Liddel [(2018)](https://link.springer.com/article/10.3758/s13423-016-1221-4) draw two distinctions with the following figure:

```{r, echo=FALSE, fig.align='left', out.width="50%", out.height="50%"}
knitr::include_graphics("img/fig1.png")
```

The estimation approach is shown in the bottom row, and it can be done within a frequentist or Bayesian workflow.

Two things are immediately striking about this. First, McElreath's entire textbook on Bayesian statistics does not involve Bayes factors (or p values, but that might have been expected). Second, taking an estimation approach is not tied to being a Bayesian. It can equally be done in a frequentist framework. I think there are theoretical and practical reasons why a Bayesian approach has advantages, but that's not important here.

So, what does this mean in practice and why do I find it appealing? 

Well, first of all, you can forget about the difficulties of interpreting the pesky p value or trying to benchmark Bayes factors. Both approaches may be perfectly fine to use (if used correctly), but you just don't need them under the estimation approach. Based on my stats background, that was a huge benefit because there was always something deeply unsatisfying about interpreting artificial bright marks or using pre-set benchmarks. It seemed to rely too heavily on categorical thinking (significant or not) or some framework for interpreting Bayes factors. All of which felt too certain for my liking.  

Instead, the estimation approach is built around estimating the magnitude and uncertainty of parameters. I instinctively found this more satisfying. It seemed a simpler starting point to summarise the distribution of a parameter (e.g., give lower and upper bounds, as well as the peak density or some other summary value) without inherently giving special status or bright lines to certain values. 

What would estimation look like?

frequentist vs Bayesian parameter plots.

It is the integrity and structure of the model and the assumptions that the model embodies that take centre stage, rather than the bright lights of a p-value or Bayes factor value.

### Build models incrementally towards the full model

Reference Barr - keep it maximal.

### Inference

Two ways to make inferences.

1.  Evaluate parameters of interest in the full model.

give an example of different posterior distributions and the extent to which they may or may not support your pre-registered analyses. e.g., two or three plots 1) overlapping zero, tightly and not so tightly. 2) Positive and tight and not so tight.

Then describe the proportion of values that span a particular range and what that would mean for your hypothesis. e.g., values as low as blah and as high as blah are compatible with your model estimates.

2.  Model comparison.

give an example. one way is to predict out-of-sample accuracy. Then compare between models. Penalty for added complexity.

In our experience in exp psych / cog neuro, model comparison has not been as helpful, but that's just our experience. They are both valuable and serve different purposes, so they may work for other labs more than others.

**Then replicate:** At this point, I would then run a replication (or a replication and extension). By doing so, you avoid joining the "Cult of the Isolated Study", which is the trap of drawing conclusions from single experiments - see [here](https://doi.org/10.2307/2981525) and [here](https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1518264).

### Pros and cons

Pros

1.  One general and yet flexible approach to data analysis
2.  Sophisticated and complex models via STAN

Cons

1.  Some models can take quite some time to build (days, weeks, months?) and need greater computer power.

### And finally (but most importantly) back to McElreath...

One of the best things about McElreath's book is that you get to the end of all this complex code and stats stuff, which made my brain hurt at times, and then he says statistical models, however advanced and complex, are fundamentally limited and need to always be framed within the wider scientific context, such as ...

- The importance of theory
- Open data and materials
- Pre-registration
- Meta-analyses
- Computational modelling
- Data science and visualisation
- Experimental design
- And many, many more besides

It is a very refreshing and humbling perspective to remember how broad, deep and diverse scientific workflows should be. 

And it made me think that traditional stats training has inadvertently led us to expect far too much certainty from statistical models. We were asking stats models to provide a level of certainty that was just not possible, given the data we were feeding into it.

### Where should I start?

If you're interested in jumping on the Bayesian estimation bandwagon, then I would do the following.

1.  Read this book first by Bodo Winter. it provides the perfect intro to estimation approaches using R. It also uses a frequentist approach, which is valuable because folks are typically more familiar with frequentist approaches than Bayesian ones and it reinforces why there is nothing Bayesian per se about estimation approaches.

2.  Read McElreath's book and watch his lectures. Take one chapter at a time and complete the code exercises. If you are more of a tidyverse person (like me) and want to use the lme4 syntax that brms provides, then follow alongside McElreath's textbook with Solomon Kurz's ebook and associated code. That's what I did.

3.  Take a look at code examples online. There are a bunch. You can find some on my publications page. But better yet, find someone else who has more experience.

4.  If you get stuck, write a reproducible example on the brms forum. Folks are really helpful on this forum and the forum seems to be fairly active.
