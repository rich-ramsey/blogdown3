---
title: "General data analysis workflow"
subtitle: | 
  A brief outline of our approach to data analysis in recent years 
date: "2023-05-10"
categories: [Data analysis, Statistics]
image: "img/statistical-rethinking.jpeg"
draft: true
author:
  - name: Richard Ramsey
    url: https://rich-ramsey.com/
    affiliation: ETH Zurich
    affiliation-url: https://ethz.ch/en.html
    orcid: 0000-0002-0329-2112
editor_options: 
  chunk_output_type: console
---

### tl:dr

-   Pre-register predictions and specific analyses in advance

-   Put p values and Bayes factors in the bin

-   Estimate parameters within (multi-level) regression models

-   Describe the posterior distribution of these parameters and evaluate them in relation to your hypotheses

### Disclaimer

I am not a statistician and I am not saying anything new here. I'm just summarising a bunch of other people's suggestions.

If I have confused things, then that's a shame. Such is life. Please send me a note with constructive feedback.

Finally, I outline one general approach to data analysis here. I have found it really refreshing compared to my past experiences and therefore I want to blog about it. But I am aware that there are many valid ways to analyse the same data, rather than there being one *correct* way. So take any suggestions below in that spirit.

### Objective

I want to outline my lab's recent approach to data analysis because others may find it helpful. It is a relatively high-level overview, rather than a step-by-step tutorial.

### Intended audience

Newbies who are looking to move towards using multi-level regression models in a Bayesian workflow. It is not going to be newsworthy for those who already have experience with estimation approaches in multi-level regression.

### My background

My background and training is in experimental psychology and cognitive neuroscience and my main research methods and statistics training will be very familiar to those from a psychology background. Each week we learned about a different "test". And each week involved some data, which was suitable to a different test. Week 1 starts with a t-test and you head towards more complex ANOVAs as the weeks move on. A version of this seems to be the cornerstone of most undergraduate and graduate statistics training worldover.

All you needed to do was pick the right test for the particular type of data and pray to the datagods that the p value would be \< 0.05. If p was \> 0.05, but still close (e.g., \< 0.1), then say something vague like "trending towards significance". If p value was \> 0.1 then just pretend that that means there is no real effect.

I became tired of this and I was looking for something different. And I had a sabbatical.

Enter Richard McElreath's book...

### Main inspiration

Richard McElreath's [textbook](https://xcelab.net/rm/statistical-rethinking/) and Solomon Kurz's [ebook](https://bookdown.org/content/4857/).

What comes below takes its primary inspiration from these resources. I should say that I am only presenting a small slice of what these books have to offer. And it may not be the most clear or coherent slice. So please read these books yourself for maximal benefit.

Two other noteworthy texts are as follows:

John Krushke's Doing Bayesian Data Analysis [textbook](https://sites.google.com/site/doingbayesiandataanalysis/)

Bodo Winter's Statistics for Linguists [textbook](https://bodowinter.com/books.html)

### Taking an estimation approach

Krushke and Liddel [(2018)](https://link.springer.com/article/10.3758/s13423-016-1221-4) draw two distinctions with the following figure:

```{r, echo=FALSE, fig.align='left', out.width="50%", out.height="50%"}
knitr::include_graphics("img/fig1.png")
```

They distinguish between hypothesis tests (top row) and Estimation with uncertainty (bottom row). They also distinguish between a frequentist approach (left column) and Bayesian approaches (right column).

Two things are immediately striking about this. First, McElreath's entire textbook on Bayesian statistics does not involve Bayes factors (or p values, but that might have been expected). Second, taking an estimation approach is not tied to being a Bayesian. It can also be done in a frequentist framework[^1].

[^1]: I think there are theoretical and practical reasons why a Bayesian approach has advantages, but that's not important here.

#### What does this mean in practice and why do I find it appealing?

Well, first of all, you can forget about the difficulties of interpreting the pesky p value or trying to benchmark Bayes factors. Both approaches may be perfectly fine to use (if used correctly), but you just don't need them under the estimation approach. Based on my stats background, that was a huge benefit because there was always something deeply unsatisfying about interpreting artificial bright marks (e.g., p \< 0.05) or using pre-set benchmarks for how to interpret Bayes factors (e.g., [these scales](https://www.statlect.com/fundamentals-of-statistics/Jeffreys-scale)). Both of these approaches seemed to rely too heavily on categorical thinking (significant or not) or some framework for interpreting Bayes factors. All of which felt too certain and too artifical for my liking.

Instead, the estimation approach is built around estimating the magnitude and uncertainty of parameters. I instinctively found this more satisfying. It seemed a simpler starting point to summarise the distribution of a parameter (e.g., give lower and upper bounds, as well as the peak density or some other summary value) without inherently giving special status or bright lines to certain values.

#### What does estimation with uncertanty look like?

Some examples may help here. Below you can see what estimation with uncertainty might look like in frequentist and Bayesian workflows.

```{r, include=FALSE}
# renv::install("tidyverse", "patchwork", "ggforce", "ggdist")
library(tidyverse)
library(patchwork)
library(ggforce)
library(ggdist)

# renv::snapshot()
```

```{r, include=FALSE}
theme_set(
  theme_bw() +
  theme(panel.grid   = element_blank(),
        panel.grid.major.y = element_line(color = alpha("firebrick4", 1/2), linetype = 3),
        axis.text.y  = element_text(hjust = 0),
        axis.ticks.y = element_blank(),
        legend.position = "none")
)
```

```{r, include=FALSE}
# simulate data
set.seed(123)
d <-  tibble(pid = 1:100,
             term = "parameter1",
             value = rnorm(100))
head(d)

# frequentist point and interval plot
p1 <-
  ggplot(data = d, aes(x = value, y = term)) +
  stat_halfeye(show_slab=FALSE, .width = .95) +
  labs(title = "Frequentist point and interval estimation") +
  coord_cartesian(xlim =c(-4, 4)) +
  scale_x_continuous(breaks=seq(-4,4,1))
p1

# bayesian density ploy
p2 <- 
  ggplot(data = d, aes(x = value, y = term)) +
  stat_halfeye(.width = .95) +
  labs(title = "Bayesian distribution estimation") +
  coord_cartesian(xlim =c(-4, 4)) +
  scale_x_continuous(breaks=seq(-4,4,1))
p2
```

```{r, echo=FALSE}
# plot them together
p3 <- p1 / p2
p3
```

For illustration purposes, this is just simulated normal distribution data. And we pretend that there is just one parameter in the model (parameter1). Let's say parameter1 reflects the difference in points on a scale between two conditions (A and B).

As you can see, the point estimate is very close to zero and the interval estimates (which are 95% confidence / quantile intervals) extend from -2 to 2. As such, the best estimate for the difference between A and B is zero, with reasonable values ranging from -2 to 2.

#### Sidenote: The benefit of Bayesian workflows - working in distributions

Although estimation with uncertainty can be done in frequentist or Bayesian workflows, there is an important difference between the two parameter estimates above. The Bayesian one has a density plot and frequentist one does not. This is important because it is crucial for thinking about the inferences that you can draw.

Even though most researchers (especially in psychology and human neuroscience) will be more familiar with frequentist thinking than Bayesian thinking, it turns out that understanding p values is not that straightforward (ref). And somewhat counterintuitively, it is more straightforward to understand the Bayesian estimation parameter that I presented above than the frequentist version (Krushke & Liddel, [2018](https://link.springer.com/article/10.3758/s13423-016-1221-4)). Try this on for size:

The frequentist interpretation:

> If I repeat this study precisely an infinite number of times, and I calculate a 95% interval each time, then 95% of those intervals will contain the true parameter.

The Bayesian interpretation:

> There is a 95% probability the parameter falls in this interval.

It seems the latter interpretation is the one that most folks tend to think they are making and definitely want to be making when in fact they are not. And that all stems from the fact that the Bayesian workflow is building a density distribution for each parameter in the model. Once you have the posterior distribution, you can just summarise it in a more straightforward manner using tools from descriptive statistics to summarise the shape and extent of the distribution.

In contrast, the frequentist interval is a flat line, such that all you can say is that 95% of an infinite number of 95% intervals will contain the true parameter. But that parameter could be anywhere along the flat line. It is no more likely to be in the middle as at the edge of the line. A density distribution is different, by definition, because it has a particular shape which, for example, may have the highest density in the middle of the distribution. This means values in the middle are more likely than at the edge, as a fucntion of the shape of the distribution. It is just a density distribution after all.

#### It is the structure of the model that matters, not the p value or Bayes factor

Ok, sidenote over. Back to the main thread.

The main emphasis in an estimation approach is different to traditional statistical thinking or commonly used approaches. At least as outlined by Richard McElreath and others, it is motivated by building multi-level regression models that try to estimate parameters that reflect reality as much as possible. As such, it is the integrity and structure of the model and the assumptions that the model embodies that take centre stage, rather than the bright lights of a p-value or Bayes factor.

This has many benefits. Instead of learning an almost infinite list of "tests" like your standard undergraduate or postgraduate statistics class, it is a more general and flexible approach that is applicable in many different contexts. You can almost always build a (multi-level) regression model to suit your purposes. This means you can have one general but powerful workflow that can be deployed in a diverse set of contexts, due to the tools freely available.

### Build models incrementally towards the full model

There are many ways to go about building models, and much debate. We will sidestep that debate and just outline our approach.

My lab takes the "keep it maximal" approach to model building (Barr et al., [2013](https://www.sciencedirect.com/science/article/pii/S0749596X12001180)). The logic here is to include all the fixed and varying (random) effects that the design permits because it tends to be a better reflection of the reality we are trying to model. Read Dale Barr's paper for more details.

Therefore, we start with an intercepts only model (i.e., one without any predictors) and build incrementally towards the full model. The full model being the one with the maximum number of fixed and varying effects that can be specified by the design.

### Implementation: lme4 vs brms (and stan)

We use brms to build the models. We are fans of the tidyverse generally and we love Solomon's ebook, which translated McElreath's R and stan code into tidyverse format and brms code. So that's what we use.

[Brms](https://paul-buerkner.github.io/brms/) is a high-level front end package for [stan](https://mc-stan.org/). Brms uses the same syntax as [lme4](https://github.com/lme4/lme4) (which is very popular in psychology). And stan is a sophisticated statistical modelling platform.

-   Advantages:
    -   Stan has far fewer failures to converge, even with complex models.
    -   Stan also has much greater flexibility in model specification.
-   Disadvantages:
    -   Computational power and time. Some Bayesian models can take a long time to run (hours, days, weeks). Only the advent of powerful desktop machines in the last 25 years or so make it even possible to run such models in a widespread manner.

### Inference

We use two approaches to make inferences.

#### 1. Evaluate parameters of interest in the full model.

We take the full model and we evaluate parameters from the full model against our pre-registered hypotheses. Our pre-registrations include the exact formula from the full model, as well as explicit hypotheses about the ways in which parameters would support hypotheses or not.

Now some examples. Take a look at some made up distributions from what could be a full model of interest.

```{r, include=FALSE}
# simulate data
set.seed(123)
d2 <-  tibble(pid = 1:100,
              parameter1 = rnorm(100, mean = 0, sd = 1),
              parameter2 = rnorm(100, mean = 0, sd = 0.1),
              parameter3 = rnorm(100, mean = 2, sd = 1),
              parameter4 = rnorm(100, mean = 2, sd = 0.1)) %>% 
  pivot_longer(-pid,
               names_to = "term",
               values_to = "value") %>% 
  mutate(pid = factor(pid),
         term = factor(term))
head(d2)
```

```{r, echo=FALSE}
# bayesian density plot
p4 <- 
  ggplot(data = d2, aes(x = value, y = term)) +
  stat_halfeye() +
  labs(title = "Bayesian distribution estimation") +
  coord_cartesian(xlim =c(-4, 4)) +
  scale_x_continuous(breaks=seq(-4,4,1))
p4
```

As you can see, these four parameters vary in their highest density (what you might take for a point estimate) and in their spread or interval. Two are centred on zero, two are centred at 2. Two are relatively broad and two are relatively narrow.

In each case, you could then describe the proportion of values that span a particular range or quantile interval. e.g., values as low as -2 and as high as 2 are compatible with your model estimates.

Then consider what this might mean in relation to your pre-registered hypotheses. Let's say for simplicity, that for all four parameters, we pre-registered that each would need to be positive to support our hypotheses. Parameter 1 and 2 would not support the hypothesis, but parameter 3 and 4 would support the hypothesis. Parameters 2 and 4 would be narrower estimates and more precise than parameters 1 and 3. Therefore, we may intially be more confident about drawing inferences from 1 and 3 versus 2 and 4.

Of course, at this point also, it would be important to evaluate what such values would mean when situated in the particular research context. If we treat these as effect sizes in original units, which they may be depending on the type of model, then what would that mean for theory or practice etc.

Ok, that's fine and fairly clear-cut, as far as data from one experiment can get you. But what about grey areas or distribtions that are a little more indeterminate in relation to our hypotheses? Take a look at the following.

```{r, include=FALSE}
# simulate data
set.seed(123)
d3 <-  tibble(pid = 1:100,
              parameter5 = rnorm(100, mean = 0.5, sd = 1),
              parameter6 = rnorm(100, mean = 1, sd = 1),
              parameter7 = rnorm(100, mean = 1.5, sd = 1),
              parameter8 = rnorm(100, mean = 2, sd = 1)) %>% 
  pivot_longer(-pid,
               names_to = "term",
               values_to = "value") %>% 
  mutate(pid = factor(pid),
         term = factor(term))
head(d3)
```

```{r, echo=FALSE}
# bayesian density plot
p5 <- 
  ggplot(data = d3, aes(x = value, y = term)) +
  stat_halfeye() +
  labs(title = "Bayesian distribution estimation") +
  coord_cartesian(xlim =c(-4, 4)) +
  scale_x_continuous(breaks=seq(-4,4,1))
p5
```

In these examples, let's again take zero as an important reference point and assume that we pre-registered a positive effect for all four parameters.

All four have the same spread. And all four have a majority of positive values. However, they vary in the strength with which they would support our hypotheses. Some have a healthy number of negative values and some have a lower bound very close to zero. Therefore, none of them provide overwhelming support for our hypotheses, at least not on their own as it stands. That being said, parameter 7 and 8 are more compatible with our hypotheses than parameter 5 and 6. That much us clear.

But, overall, more experiments and data would be needed. This is true in all cases in my view, but in this in particular because of the indeterminate nature of the distributions. Maybe it would be time to increase the precision by collecting more data and see if a replciation can make the estimates more precise.

#### 2. Model comparison.

give an example. one way is to predict out-of-sample accuracy. Then compare between models. Penalty for added complexity.

Note: depending on your research question and type of data, the relative merits and value of parameter estimation versus model comparison may differ. For example, in our experience in exp psych / cog neuro, model comparison has not been as helpful, but that's just our experience. They are both valuable and serve different purposes, so they may work for other labs, questions, datasets more than others.

**Then replicate:** At this point, I would then run a replication (or a replication and extension). By doing so, you avoid joining the "Cult of the Isolated Study", which is the trap of drawing conclusions from single experiments - see [here](https://doi.org/10.2307/2981525) and [here](https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1518264).

### Pros and cons

Pros

1.  One general and yet flexible approach to data analysis
2.  Sophisticated and complex models via STAN

Cons

1.  Some models can take quite some time to build (days, weeks, months?) and need greater computer power.

### Implications

-   Compared to more conventional stats training
    -   Much more information is used and presented
    -   Assumptions reflect what we know about human cognition and behaviour i.e., complex, variable (across people and situations) and multi-levelled.
    -   Bespoke models need building and choices need justifying -- so you gotta think a bit...
-   Implications for the replication crisis?
    -   There are fewer places to hide, due to a more complete presentation of information
    -   It changes what a replication attempt would even look like e.g., p \< 0.05 is not enough
    -   More nuanced interpretations are required.
    -   It naturally encourages considerations regarding generalisability (across people, stimuli, places etc.) by virtue of taking a maximal varying structure approach.

### And finally (but most importantly) back to McElreath...

One of the best things about McElreath's book is that you get to the end of all this complex code and stats stuff, which made my brain hurt at times, and then he says statistical models, however advanced and complex, are fundamentally limited and need to always be framed within the wider scientific context, such as ...

-   The importance of theory
-   Open data and materials
-   Pre-registration
-   Meta-analyses
-   Computational modelling
-   Data science and visualisation
-   Experimental design
-   And many, many more besides

It is a very refreshing and humbling perspective to remember how broad, deep and diverse scientific workflows should be.

And it made me realise how traditional stats training had inadvertently led us to expect far too much certainty from statistical models. We were asking statistical models to provide a level of certainty that was just not possible. And so it was time for a re-set.

### Where should I start?

If you're interested in jumping on the Bayesian estimation bandwagon, then I would do the following.

1.  Read this book first by Bodo Winter. It provides the perfect intro to estimation approaches using R. It also uses a frequentist approach, which is valuable because folks are typically more familiar with frequentist approaches than Bayesian ones and it reinforces why there is nothing Bayesian per se about estimation approaches.

2.  Read McElreath's book and watch his lectures. Take one chapter at a time and complete the code exercises. If you are more of a tidyverse person (like me) and want to use the lme4 syntax that brms provides, then follow alongside McElreath's textbook with Solomon Kurz's ebook and associated code. That's what I did.

3.  Take a look at code examples online from other people's work. There are a bunch. You can find some on my [publications page](https://rich-ramsey.com/publications/publications.html) and [OSF page](https://osf.io/s67e2/). But better yet, find someone else who has more experience like Solomon Kurz and many, many more people out there.

4.  If you get stuck, write a reproducible example on the brms forum. Folks are really helpful on this forum and the forum seems to be fairly active.
